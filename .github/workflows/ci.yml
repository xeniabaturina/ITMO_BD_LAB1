name: CI Pipeline

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]

jobs:
  build-test-push:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch all history for DVC
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest coverage dvc
        
    - name: Setup DVC
      run: |
        # Skip DVC init if .dvc directory already exists
        if [ ! -d ".dvc" ]; then
          dvc init --no-scm
        else
          echo "DVC already initialized, skipping init"
        fi
        
        # Configure DagsHub as the remote storage
        dvc remote modify dagshub url https://dagshub.com/xeniabaturina/ITMO_BD_LAB1.dvc
        
        # Set up authentication for DagsHub
        echo "machine dagshub.com login ${{ secrets.DAGSHUB_USERNAME }} password ${{ secrets.DAGSHUB_TOKEN }}" > ~/.netrc
        
        # Try to pull data from DagsHub
        dvc pull -v || {
          echo "Failed to pull data from DagsHub, using sample data instead"
          # For CI/CD testing, we use a sample dataset if DVC pull fails
          mkdir -p data
          curl -o data/penguins.csv https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv
        }
        
    - name: Create data directories
      run: |
        mkdir -p data
        
    - name: Verify data files
      run: |
        # Check if data files exist
        if [ ! -f "data/penguins.csv" ]; then
          echo "Creating sample data for testing..."
          # Download the Palmer Penguins dataset
          curl -o data/penguins.csv https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv
        else
          echo "Data files already exist, skipping download"
        fi
        
    - name: Create GitHub Actions config.ini
      run: |
        cat > config.ini << EOF
        [DATA]
        x_data = data/Penguins_X.csv
        y_data = data/Penguins_y.csv
        
        [SPLIT_DATA]
        x_train = data/Train_Penguins_X.csv
        y_train = data/Train_Penguins_y.csv
        x_test = data/Test_Penguins_X.csv
        y_test = data/Test_Penguins_y.csv
        
        [RANDOM_FOREST]
        n_estimators = 100
        max_depth = None
        min_samples_split = 2
        min_samples_leaf = 1
        path = experiments/random_forest.sav
        EOF
        
    - name: Prepare data
      run: |
        mkdir -p experiments
        python src/preprocess.py
        
        # Verify data files were created
        ls -la data/
        
    - name: Run tests
      run: |
        python -m pytest src/unit_tests/
        
    - name: Calculate test coverage
      run: |
        python -m coverage run -m pytest src/unit_tests/
        python -m coverage report
        python -m coverage html
        
    - name: Train model
      run: |
        python src/train.py
        
        # Verify model file was created
        ls -la experiments/
        
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
      
    - name: Login to DockerHub
      if: github.event_name != 'pull_request'
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKERHUB_USERNAME }}
        password: ${{ secrets.DOCKERHUB_TOKEN }}
        
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: ${{ github.event_name != 'pull_request' }}
        tags: ${{ secrets.DOCKERHUB_USERNAME }}/penguin-classifier:latest,${{ secrets.DOCKERHUB_USERNAME }}/penguin-classifier:${{ github.sha }}
        
    - name: Update dev_sec_ops.yml
      if: github.event_name != 'pull_request'
      run: |
        # Get Docker image digest
        DIGEST=$(docker inspect --format='{{index .RepoDigests 0}}' ${{ secrets.DOCKERHUB_USERNAME }}/penguin-classifier:latest | cut -d'@' -f2)
        
        # Get last 5 commit hashes
        COMMITS=$(git log -5 --pretty=format:"%H %s")
        
        # Update dev_sec_ops.yml with the new digest and commits
        sed -i "s|digest: sha256:.*|digest: $DIGEST|" dev_sec_ops.yml
        
        # Get test coverage
        COVERAGE=$(python -m coverage report | grep TOTAL | awk '{print $NF}')
        sed -i "s|total: .*%|total: $COVERAGE|" dev_sec_ops.yml
        
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: model-and-reports
        path: |
          experiments/
          htmlcov/
          dev_sec_ops.yml 